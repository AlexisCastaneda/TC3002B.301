{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ui50qOngQkoT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "folder_path_plg = \"/content/drive/MyDrive/DATASET_IA/documentos-con texto de otros/\"\n",
        "# Obtener los nombres de los archivos en la carpeta\n",
        "file_names_plg = glob.glob(folder_path_plg + \"/*\")\n",
        "\n",
        "folder_path_gen = \"/content/drive/MyDrive/DATASET_IA/documentos-genuinos/\"\n",
        "# Obtener los nombres de los archivos en la carpeta\n",
        "file_names_gen = glob.glob(folder_path_gen + \"/*\")"
      ],
      "metadata": {
        "id": "5RnyYwWpZt8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Dataset from originals\n",
        "original_texts_nf = []\n",
        "for file_path in file_names_gen:\n",
        "  with open(file_path, \"r\") as file:\n",
        "      content = file.read()\n",
        "      original_texts_nf.append(content)\n",
        "\n"
      ],
      "metadata": {
        "id": "zobNOG3_S2iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Imprimir los nombres de los archivos\n",
        "plagiarized_texts_nf = []\n",
        "for file_name in file_names_plg:\n",
        "  with open(file_name, \"r\") as file:\n",
        "      content = file.read()\n",
        "      plagiarized_texts_nf.append(content)\n",
        "\n",
        "print(len(plagiarized_texts_nf))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gd66zGJWUjb3",
        "outputId": "49bde574-e356-4bd3-b449-c93802be16bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def limpiarText(texto_sucio):\n",
        "  text = re.sub(r\"[^\\w\\s]\", \"\", texto_sucio)\n",
        "  text = re.sub(r\"\\d+\", \"\", text)\n",
        "  text = text.replace(\"\\n\", \" \")\n",
        "  textoLimpio = text.lower()\n",
        "\n",
        "  #textoLimpio = re.findall(r'\\w+\\W*|\\w+', texto_sucio)\n",
        "  return textoLimpio\n",
        "\n",
        "original_texts = []\n",
        "\n",
        "for o in original_texts_nf:\n",
        "  original_texts.append(limpiarText(o))\n",
        "\n",
        "plagiarized_texts = []\n",
        "for o in plagiarized_texts_nf:\n",
        "  plagiarized_texts.append(limpiarText(o))\n",
        "\n",
        "#print(len(original_texts))\n",
        "print(len(plagiarized_texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FY0weRAkaXnj",
        "outputId": "3212f988-7e16-45a8-bd09-d077c5993b3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Combine original and plagiarized texts\n",
        "texts = original_texts + plagiarized_texts\n",
        "\n",
        "# Create labels (0 for original, 1 for plagiarized)\n",
        "labels = np.array([0] * len(original_texts) + [1] * len(plagiarized_texts))\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "max_sequence_length = max(len(sequence) for sequence in sequences)\n",
        "sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Split data into train and test sets\n",
        "train_size = int(0.8 * len(texts))\n",
        "train_sequences = sequences[:train_size]\n",
        "train_labels = labels[:train_size]\n",
        "test_sequences = sequences[train_size:]\n",
        "test_labels = labels[train_size:]\n"
      ],
      "metadata": {
        "id": "HjXP0f2ORMI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Build the model\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Embedding(len(tokenizer.word_index) + 1, 100, input_length=max_sequence_length),\n",
        "    keras.layers.GlobalMaxPooling1D(),\n",
        "    keras.layers.Dense(16, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "lDBugVOcRS9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(train_sequences, train_labels, epochs=100, batch_size=15, validation_data=(test_sequences, test_labels),\n",
        "          callbacks=[keras.callbacks.EarlyStopping(patience=5)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMf9gKCZRWZi",
        "outputId": "a71f7f05-fe2a-4772-a575-efbc932c3049"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "12/12 [==============================] - 2s 142ms/step - loss: 0.5408 - accuracy: 0.6857 - val_loss: 0.9869 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 2s 185ms/step - loss: 0.5185 - accuracy: 0.6857 - val_loss: 0.9487 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 2s 142ms/step - loss: 0.4924 - accuracy: 0.6857 - val_loss: 0.8953 - val_accuracy: 0.0227\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 3s 253ms/step - loss: 0.4629 - accuracy: 0.7029 - val_loss: 0.8235 - val_accuracy: 0.1364\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 2s 176ms/step - loss: 0.4267 - accuracy: 0.7657 - val_loss: 0.7795 - val_accuracy: 0.3182\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 2s 140ms/step - loss: 0.3925 - accuracy: 0.8743 - val_loss: 0.6599 - val_accuracy: 0.7045\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 1s 102ms/step - loss: 0.3510 - accuracy: 0.9371 - val_loss: 0.6043 - val_accuracy: 0.7273\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 2s 139ms/step - loss: 0.3125 - accuracy: 0.9486 - val_loss: 0.5515 - val_accuracy: 0.7273\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 2s 122ms/step - loss: 0.2765 - accuracy: 0.9600 - val_loss: 0.4678 - val_accuracy: 0.8864\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 1s 106ms/step - loss: 0.2428 - accuracy: 0.9771 - val_loss: 0.4213 - val_accuracy: 0.9091\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 1s 107ms/step - loss: 0.2109 - accuracy: 0.9886 - val_loss: 0.3673 - val_accuracy: 0.9091\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 2s 152ms/step - loss: 0.1846 - accuracy: 0.9886 - val_loss: 0.3207 - val_accuracy: 0.9091\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 2s 168ms/step - loss: 0.1615 - accuracy: 0.9943 - val_loss: 0.2766 - val_accuracy: 0.9545\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - 1s 86ms/step - loss: 0.1417 - accuracy: 0.9943 - val_loss: 0.2530 - val_accuracy: 0.9545\n",
            "Epoch 15/100\n",
            "12/12 [==============================] - 1s 123ms/step - loss: 0.1247 - accuracy: 0.9943 - val_loss: 0.2203 - val_accuracy: 0.9545\n",
            "Epoch 16/100\n",
            "12/12 [==============================] - 1s 63ms/step - loss: 0.1103 - accuracy: 0.9943 - val_loss: 0.1939 - val_accuracy: 0.9545\n",
            "Epoch 17/100\n",
            "12/12 [==============================] - 1s 85ms/step - loss: 0.0987 - accuracy: 0.9943 - val_loss: 0.1778 - val_accuracy: 0.9545\n",
            "Epoch 18/100\n",
            "12/12 [==============================] - 1s 123ms/step - loss: 0.0883 - accuracy: 0.9943 - val_loss: 0.1647 - val_accuracy: 0.9545\n",
            "Epoch 19/100\n",
            "12/12 [==============================] - 1s 120ms/step - loss: 0.0800 - accuracy: 0.9943 - val_loss: 0.1413 - val_accuracy: 0.9545\n",
            "Epoch 20/100\n",
            "12/12 [==============================] - 1s 63ms/step - loss: 0.0722 - accuracy: 0.9943 - val_loss: 0.1325 - val_accuracy: 0.9545\n",
            "Epoch 21/100\n",
            "12/12 [==============================] - 1s 65ms/step - loss: 0.0653 - accuracy: 0.9943 - val_loss: 0.1205 - val_accuracy: 1.0000\n",
            "Epoch 22/100\n",
            "12/12 [==============================] - 1s 64ms/step - loss: 0.0603 - accuracy: 1.0000 - val_loss: 0.0998 - val_accuracy: 1.0000\n",
            "Epoch 23/100\n",
            "12/12 [==============================] - 1s 128ms/step - loss: 0.0546 - accuracy: 1.0000 - val_loss: 0.0929 - val_accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "12/12 [==============================] - 1s 69ms/step - loss: 0.0511 - accuracy: 1.0000 - val_loss: 0.0980 - val_accuracy: 1.0000\n",
            "Epoch 25/100\n",
            "12/12 [==============================] - 1s 99ms/step - loss: 0.0465 - accuracy: 1.0000 - val_loss: 0.0892 - val_accuracy: 1.0000\n",
            "Epoch 26/100\n",
            "12/12 [==============================] - 1s 120ms/step - loss: 0.0433 - accuracy: 1.0000 - val_loss: 0.0736 - val_accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "12/12 [==============================] - 1s 104ms/step - loss: 0.0406 - accuracy: 1.0000 - val_loss: 0.0675 - val_accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "12/12 [==============================] - 1s 86ms/step - loss: 0.0376 - accuracy: 1.0000 - val_loss: 0.0622 - val_accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "12/12 [==============================] - 1s 66ms/step - loss: 0.0354 - accuracy: 1.0000 - val_loss: 0.0634 - val_accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "12/12 [==============================] - 1s 65ms/step - loss: 0.0329 - accuracy: 1.0000 - val_loss: 0.0604 - val_accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "12/12 [==============================] - 1s 47ms/step - loss: 0.0312 - accuracy: 1.0000 - val_loss: 0.0578 - val_accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "12/12 [==============================] - 1s 85ms/step - loss: 0.0292 - accuracy: 1.0000 - val_loss: 0.0528 - val_accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "12/12 [==============================] - 1s 68ms/step - loss: 0.0278 - accuracy: 1.0000 - val_loss: 0.0461 - val_accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "12/12 [==============================] - 1s 46ms/step - loss: 0.0264 - accuracy: 1.0000 - val_loss: 0.0473 - val_accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "12/12 [==============================] - 1s 85ms/step - loss: 0.0249 - accuracy: 1.0000 - val_loss: 0.0440 - val_accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "12/12 [==============================] - 1s 66ms/step - loss: 0.0237 - accuracy: 1.0000 - val_loss: 0.0422 - val_accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "12/12 [==============================] - 1s 65ms/step - loss: 0.0223 - accuracy: 1.0000 - val_loss: 0.0406 - val_accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "12/12 [==============================] - 1s 126ms/step - loss: 0.0214 - accuracy: 1.0000 - val_loss: 0.0377 - val_accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "12/12 [==============================] - 1s 100ms/step - loss: 0.0205 - accuracy: 1.0000 - val_loss: 0.0345 - val_accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "12/12 [==============================] - 2s 145ms/step - loss: 0.0196 - accuracy: 1.0000 - val_loss: 0.0357 - val_accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "12/12 [==============================] - 1s 82ms/step - loss: 0.0188 - accuracy: 1.0000 - val_loss: 0.0298 - val_accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "12/12 [==============================] - 0s 25ms/step - loss: 0.0178 - accuracy: 1.0000 - val_loss: 0.0283 - val_accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 0.0169 - accuracy: 1.0000 - val_loss: 0.0292 - val_accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0164 - accuracy: 1.0000 - val_loss: 0.0316 - val_accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "12/12 [==============================] - 1s 102ms/step - loss: 0.0157 - accuracy: 1.0000 - val_loss: 0.0271 - val_accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "12/12 [==============================] - 1s 46ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.0255 - val_accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.0256 - val_accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "12/12 [==============================] - 1s 84ms/step - loss: 0.0140 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "12/12 [==============================] - 1s 84ms/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.0213 - val_accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "12/12 [==============================] - 1s 84ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.0214 - val_accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "12/12 [==============================] - 1s 47ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.0217 - val_accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "12/12 [==============================] - 0s 45ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.0204 - val_accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "12/12 [==============================] - 1s 66ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.0206 - val_accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "12/12 [==============================] - 1s 46ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.0190 - val_accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "12/12 [==============================] - 1s 45ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.0174 - val_accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "12/12 [==============================] - 1s 70ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.0175 - val_accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "12/12 [==============================] - 0s 41ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.0181 - val_accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.0165 - val_accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "12/12 [==============================] - 1s 73ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.0163 - val_accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "12/12 [==============================] - 1s 71ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.0157 - val_accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "12/12 [==============================] - 1s 90ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.0149 - val_accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "12/12 [==============================] - 1s 45ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.0147 - val_accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "12/12 [==============================] - 1s 66ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.0137 - val_accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 0.0137 - val_accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "12/12 [==============================] - 1s 65ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.0138 - val_accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "12/12 [==============================] - 1s 63ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.0130 - val_accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "12/12 [==============================] - 1s 25ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.0120 - val_accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "12/12 [==============================] - 1s 65ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.0121 - val_accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "12/12 [==============================] - 1s 46ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0112 - val_accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0118 - val_accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "12/12 [==============================] - 1s 86ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.0103 - val_accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0105 - val_accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.0107 - val_accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "12/12 [==============================] - 1s 47ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0101 - val_accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0097 - val_accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "12/12 [==============================] - 0s 45ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.0096 - val_accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "12/12 [==============================] - 1s 46ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.0093 - val_accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.0091 - val_accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "12/12 [==============================] - 1s 66ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0088 - val_accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "12/12 [==============================] - 0s 27ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "12/12 [==============================] - 1s 46ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.0085 - val_accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0081 - val_accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0084 - val_accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0074 - val_accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "12/12 [==============================] - 1s 42ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.0073 - val_accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "12/12 [==============================] - 1s 71ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0070 - val_accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.0068 - val_accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.0069 - val_accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "12/12 [==============================] - 0s 38ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0066 - val_accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "12/12 [==============================] - 1s 103ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0063 - val_accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0059 - val_accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0063 - val_accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0057 - val_accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0057 - val_accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "12/12 [==============================] - 0s 29ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0056 - val_accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "12/12 [==============================] - 1s 47ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "12/12 [==============================] - 1s 46ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0055 - val_accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "12/12 [==============================] - 1s 46ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0055 - val_accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8059448dc0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "_, accuracy = model.evaluate(test_sequences, test_labels)\n",
        "print('Accuracy:', accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oj-64ldKRYww",
        "outputId": "aad713c1-3395-408e-a1dc-051b02d871b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 7ms/step - loss: 0.0055 - accuracy: 1.0000\n",
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict plagiarism\n",
        "def detect_plagiarism(text):\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    sequence = pad_sequences(sequence, maxlen=max_sequence_length)\n",
        "    prediction = model.predict(sequence)[0][0]\n",
        "    print(prediction)\n",
        "    if prediction < 0.5:\n",
        "        return \"Original\"\n",
        "    else:\n",
        "        return \"Plagiarized\"\n",
        "\n",
        "# Example usage\n",
        "input_text = \"How to measure similarity between two sequences is a key question in information theory and in computer science. Based on Kolmogorov complexity, we have proposed a metric to answer this question and prove it to be universal. A substantial tool is the documents similarity metric applied in fields such as plagiarism detection, problems necessary to capture the semantic, in relation to documents, determining the topic, or structural, or syntactic likeness of texts. To evaluate the proposed method, we construct a comprehensive dataset comprising documents from diverse domains, including academic research papers, news articles, and online forums. We simulate various plagiarism scenarios, such as paraphrasing, sentence reordering, and word substitution, to assess the robustness and effectiveness of our approach. Of different linguistic patterns in plagiarism committing, the taxonomy deep understanding supports, an example is example, texts changing into semantically equivalent but with different organization, texts shortening with concept specification and generalization, and ideas adopting, and of others, key contributions.\"\n",
        "input_text2 = \"There are many variations of passages of Lorem Ipsum available, but the majority have suffered alteration in some form, by injected humour, or randomised words which don't look even slightly believable. If you are going to use a passage of Lorem Ipsum, you need to be sure there isn't anything embarrassing hidden in the middle of text. All the Lorem Ipsum generators on the Internet tend to repeat predefined chunks as necessary, making this the first true generator on the Internet. It uses a dictionary of over 200 Latin words, combined with a handful of model sentence structures, to generate Lorem Ipsum which looks reasonable. The generated Lorem Ipsum is therefore always free from repetition, injected humour, or non-characteristic words etc.\"\n",
        "input_text3 = \"Plagiarism of digital documents seems a serious problem in today's era. Plagiarism refers to the use of someone's data, language and writing without proper acknowledgment of the original source. Plagiarism of another author's original work is one of the biggest problems in publishing, science, and education. Plagiarism can be of different types. This paper presents a different approach for measuring semantic similarity between words and their meanings. Existing systems are based on the traditional approach. This approach offers the opportunity to tie ethical issues to technical content at the moment students learn it, and to have students engage with these issues repeatedly throughout their degree. However, little is known about the effect of embedded ethics education on students. Therefore, two algorithms are presented: BERT (Bidirectional Encoder Representations from Transformers) which uses a neural network to process the natural language of the texts to capture the context of the words and understand the relationships between them as well as the algorithm USE (Universal Sentence Encoder) which converts sentences into vector representations to perform a semantic analysis.\"\n",
        "input_text4 = \"Our results suggest that the mean similarity index among postgraduate theses in Austria is 8.78 (Standard deviation 4.91), while the mean similarity index among graduate theses prepared in Turkey is 25.10 ( Standard deviation 9.85). In fact, our analysis indicates that 91% (n 113) of theses prepared at Austrian universities and only 13% (n 17) of theses prepared at Turkish universities failed the acceptable similarity rate of 15%. The fact that 87% of the theses written in Turkey are dramatically similar to the available resources shows that many of the studies carry potential risks in terms of originality and plagiarism. In this article, we discuss the architecture of the APAS web application with a focus on scalability issues. We review foundational features like dynamic analysis and unreliable code execution, as well as more complex cases like static analysis and plagiarism detection, and summarize the lessons learned from the previous six years of research. We identify scalability challenges, show how they have been addressed in APAS Edgar, and then propose general architectural solutions, building blocks, and patterns to address those challenges.\"\n",
        "input_text5 = \"It describes the experiment of building a software capable of generating leads and newspaper titles in an automated fashion from information obtained from the Internet. The theoretical possibility Lage already provided by the end of last century is based on relatively rigid and simple structure of this type of story construction, which facilitates the representation or translation of its syntax in terms of instructions that the computer can execute. The paper also discusses the relationship between society, technique and technology, making a brief history of the introduction of digital solutions in newsrooms and their impacts. The development was done with the Python programming language and NLTK- Natural Language Toolkit library - and used the results of the Brazilian Soccer Championship 2013 published on an internet portal as a data source.\"\n",
        "input_text6 = \"Plagiarism of digital documents seems a serious problem in today's era. Plagiarism refers to the use of someone's data, language and writing without proper acknowledgment of the original source. Plagiarism of another author's original work is one of the biggest problems in publishing, science, and education. Plagiarism can be of different types. This paper presents a different approach for measuring semantic similarity between words and their meanings. Existing systems are based on the traditional approach. This approach offers the opportunity to tie ethical issues to technical content at the moment students learn it, and to have students engage with these issues repeatedly throughout their degree. However, little is known about the effect of embedded ethics education on students. Therefore, two algorithms are presented: BERT (Bidirectional Encoder Representations from Transformers) which uses a neural network to process the natural language of the texts to capture the context of the words and understand the relationships between them as well as the algorithm USE (Universal Sentence Encoder) which converts sentences into vector representations to perform a semantic analysis.\"\n",
        "input_text7 = \"Short text similarity plays an important role in natural language processing (NLP). It has been applied in many fields. Due to the lack of sufficient context in the short text, it is difficult to measure the similarity. The use of semantics similarity to calculate textual similarity has attracted the attention of academia and industry and achieved better results. In this survey, we have conducted a comprehensive and systematic analysis of semantic similarity. We first propose three categories of semantic similarity: corpus-based, knowledge-based, and deep learning (DL)-based. We analyze the pros and cons of representative and novel algorithms in each category. Our analysis also includes the applications of these similarity measurement methods in other areas of NLP. We then evaluate state-of-the-art DL methods on four common datasets, which proved that DL-based can better solve the challenges of the short text similarity, such as sparsity and complexity. Especially, bidirectional encoder representations from transformer model can fully employ scarce information of short texts and semantic information and obtain higher accuracy and F1 value. We finally put forward some future directions.\"\n",
        "\n",
        "result = detect_plagiarism(input_text)\n",
        "print(\"Result:\", result)\n",
        "\n",
        "result2 = detect_plagiarism(input_text2)\n",
        "print(\"Result:\", result2)\n",
        "\n",
        "result3 = detect_plagiarism(input_text3)\n",
        "print(\"Result:\", result3)\n",
        "\n",
        "result4 = detect_plagiarism(input_text4)\n",
        "print(\"Result:\", result4)\n",
        "\n",
        "result5 = detect_plagiarism(input_text5)\n",
        "print(\"Result:\", result5)\n",
        "\n",
        "result6 = detect_plagiarism(input_text6)\n",
        "print(\"Result:\", result6)\n",
        "\n",
        "result7 = detect_plagiarism(input_text7)\n",
        "print(\"Result:\", result7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKujtMRlRa3j",
        "outputId": "7012e1f8-6e74-41cc-c050-97569d689ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 88ms/step\n",
            "0.99931335\n",
            "Result: Plagiarized\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "0.09468009\n",
            "Result: Original\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "0.9976822\n",
            "Result: Plagiarized\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "0.99790883\n",
            "Result: Plagiarized\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "3.281298e-05\n",
            "Result: Original\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "0.9976822\n",
            "Result: Plagiarized\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "0.008914871\n",
            "Result: Original\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path_gen = \"/content/drive/MyDrive/DATASET_IA/docmentos-sospechosos\"\n",
        "# Obtener los nombres de los archivos en la carpeta\n",
        "file_names_gen = glob.glob(folder_path_gen + \"/*\")\n",
        "\n",
        "\n",
        "#Dataset from originals\n",
        "pruebas = []\n",
        "for file_path in file_names_gen:\n",
        "  print(file_path)\n",
        "  with open(file_path, \"r\") as file:\n",
        "      content = file.read()\n",
        "      pruebas.append(content)\n",
        "\n",
        "for t in pruebas:\n",
        "  result = detect_plagiarism(limpiarText(t))\n",
        "  print(\"Result:\", result)\n",
        "\n",
        "print(\"SIN TEXTO LIMPIO\")\n",
        "\n",
        "for t in pruebas:\n",
        "  result = detect_plagiarism(t)\n",
        "  print(\"Result:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWzkXwYIvBtq",
        "outputId": "f039f5a3-e029-4e72-eef8-dff177358e4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/DATASET_IA/docmentos-sospechosos/FID-12.txt\n",
            "/content/drive/MyDrive/DATASET_IA/docmentos-sospechosos/FID-04.txt\n",
            "/content/drive/MyDrive/DATASET_IA/docmentos-sospechosos/FID-15.txt\n",
            "/content/drive/MyDrive/DATASET_IA/docmentos-sospechosos/FID-14.txt\n",
            "/content/drive/MyDrive/DATASET_IA/docmentos-sospechosos/FID-08.txt\n",
            "/content/drive/MyDrive/DATASET_IA/docmentos-sospechosos/FID-07.txt\n",
            "/content/drive/MyDrive/DATASET_IA/docmentos-sospechosos/FID-01.txt\n",
            "/content/drive/MyDrive/DATASET_IA/docmentos-sospechosos/FID-11.txt\n",
            "/content/drive/MyDrive/DATASET_IA/docmentos-sospechosos/FID-06.txt\n",
            "/content/drive/MyDrive/DATASET_IA/docmentos-sospechosos/FID-05.txt\n",
            "/content/drive/MyDrive/DATASET_IA/docmentos-sospechosos/FID-03.txt\n",
            "/content/drive/MyDrive/DATASET_IA/docmentos-sospechosos/FID-13.txt\n",
            "/content/drive/MyDrive/DATASET_IA/docmentos-sospechosos/FID-09.txt\n",
            "/content/drive/MyDrive/DATASET_IA/docmentos-sospechosos/FID-10.txt\n",
            "/content/drive/MyDrive/DATASET_IA/docmentos-sospechosos/FID-02.txt\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "0.0017402323\n",
            "Result: Original\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "0.0034965116\n",
            "Result: Original\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "0.9987985\n",
            "Result: Plagiarized\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "0.008441575\n",
            "Result: Original\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "0.024740964\n",
            "Result: Original\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "0.9992569\n",
            "Result: Plagiarized\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "0.9975122\n",
            "Result: Plagiarized\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "0.9388829\n",
            "Result: Plagiarized\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "0.01909243\n",
            "Result: Original\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "0.9991923\n",
            "Result: Plagiarized\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "0.99931335\n",
            "Result: Plagiarized\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "0.99936587\n",
            "Result: Plagiarized\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "0.9994691\n",
            "Result: Plagiarized\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "0.0004285974\n",
            "Result: Original\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "0.012807075\n",
            "Result: Original\n",
            "SIN TEXTO LIMPIO\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "0.0019604315\n",
            "Result: Original\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "0.002072485\n",
            "Result: Original\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "0.9976822\n",
            "Result: Plagiarized\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "0.008914871\n",
            "Result: Original\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "0.028184026\n",
            "Result: Original\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "0.9969085\n",
            "Result: Plagiarized\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "0.98936814\n",
            "Result: Plagiarized\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "0.9388846\n",
            "Result: Plagiarized\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "0.023196379\n",
            "Result: Original\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "0.9991923\n",
            "Result: Plagiarized\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "0.99931335\n",
            "Result: Plagiarized\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "0.994531\n",
            "Result: Plagiarized\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "0.9994691\n",
            "Result: Plagiarized\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "0.0005045802\n",
            "Result: Original\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "0.01227797\n",
            "Result: Original\n"
          ]
        }
      ]
    }
  ]
}